<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Weijian (William) Luo, PhD</title>
<meta name="description" content="Weijian Luo's academic website.">

<!-- Open Graph --> 


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="main.css">

<link rel="canonical" href="/">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center">
  <a href="mailto:pkulwj1994@icloud.com"><i class="fas fa-envelope"></i></a>
  <a href="https://scholar.google.com/citations?hl=zh-CN&user=kAYjIR4AAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>  
  <a href="https://github.com/pkulwj1994" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
<!--   <a href="https://www.linkedin.com/in/xxx/" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
<!--   <a href="https://twitter.com/xxx" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a> -->
  
  
  
  
</span>

        </div>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="index.html">
              About
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>

          <li class="nav-item ">
              <a class="nav-link" href="publications.html">
                Publications
                
              </a>
          </li>

          <li class="nav-item ">
              <a class="nav-link" href="CV.pdf">
                CV                
              </a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Weijian Luo, PhD</span>
    </h1>
    
    <!-- 添加个人座右铭 -->
    <div class="motto-section text-center mt-3 mb-4">
      <p class="motto-text font-italic" style="font-size: 1.0rem; color: #555; border-left: 3px solid #007bff; padding-left: 15px;">
        "My mission is to build an ecosystem where AI and human beings interact<br>
        in <strong>harmonious, efficient, and healthy</strong> ways that continuously provides value to human society."
      </p>
    </div>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="./photos/head.jpg">
      
      
    </div>
    
    <div class="clearfix">

      <p> <strong>Weijian</strong> is a <strong><a href="https://job.xiaohongshu.com/campus/redstar">RedStar</a></strong> Senior research scientist of <strong>Humane Intelligence (hi) lab</strong> of <strong><a href="https://job.xiaohongshu.com">Xiaohongshu (RedNote) Inc</a></strong>, Beijing. Currently, he leads the research team of <strong>large generative understanding foundation models</strong> in <strong>hi-lab</strong>. Weijian obtained his Doctoral Degree in Statistics and Generative Modeling from the School of Mathematical Sciences, <strong><a href="http://english.pku.edu.cn">Peking University</a></strong>. He received his M.S. Degree in Applied Statistics from the School of Mathematical Sciences also from <strong><a href="http://english.pku.edu.cn">Peking University</a></strong>, and his B.S. degree in Mathematics from <strong><a href="http://en.ustc.edu.cn/">University of Science and Technology of China (USTC)</a></strong>.</p>
      
      <p> <strong>Research Interests:</strong> Weijian's early work had set the theory and practices for modern one-step text-to-image generative models. Currently, Weijian leads the research team of <strong>large generative understanding models</strong> in <strong>hi-lab</strong>. His team focuses on developing cutting-edge, efficient generative understanding models that can reason, understand humane intentions, and generate vision-audio responses in a real-time manner. Weijian also leads the research direction of next-generation generative models, including one-step text-to-image and video models at scale.</p>
      
      <p> <strong>Call for Talents:</strong> Weijian's team in Beijing is actively hiring talented research scientists and engineers. The team encourages candidates with strong track records and unparalleled curiosity about next-gen generative understanding models to apply for the <strong><a href="https://job.xiaohongshu.com/campus/redstar">RedStar Research Scientist program</a></strong>, as well as the <strong><a href="https://job.xiaohongshu.com/campus/landing/top_intern">ACE intern program</a></strong>.</p>
      
      <p> <strong>Academic Services:</strong> Weijian is invited as a reviewer for academic journals including <strong>Nature Communications <a href="https://www.nature.com/ncomms/">(NC)</a></strong>, <strong>Journal of Machine Learning Research <a href="https://www.jmlr.org/">(JMLR)</a></strong>, IEEE Transactions on Image Processing <strong><a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-image-processing">(TIP)</a></strong>, IEEE Transactions on Neural Networks and Learning Systems <strong><a href="https://cis.ieee.org/publications/t-neural-networks-and-learning-systems">(TNNLS)</a></strong>, and Pattern Recognition <strong><a href="https://www.sciencedirect.com/journal/pattern-recognition">(PR)</a></strong>. He also reviews for top AI Conferences including <strong>NeurIPS, ICML, ICLR, CVPR, ICCV, AISTATS, UAI, ACM-MM</strong>, etc;</p>

      <p> <strong>Contact:</strong> <span style="font-family:'Lucida Console', monospace">pkulwj1994 at icloud dot com</span></p>

<p><strong>Selected Talks:</strong></p>
<ul>
  
<li> <strong>Google Deepmind Research</strong> invited me to deliver a talk in 12th Nov, 2024 on one-step cross-modality generative models. Please check out the slides through <a href="https://drive.google.com/file/d/1NrhHBKtVcZ8Jf2-I8v_QKz-Xsdvfqof0/view?usp=sharing">A Path to Human-preferred One-step Text-to-image Generative Models</a>.
</li>

<li> The <a href="https://www.x-agi.cc/schedule.html"><strong> 18th X-AGI && China-R Conference</strong></strong></a> invited me to deliver a talk in the multimodal panel, 18th, October 2025. The title of the talk is <a href="https://drive.google.com/file/d/1cSDHniBoeanTP56PCodO_SZ1ozW8cMT9/view?usp=sharing">Multimodal Generation and Understanding: the Evolution of Data and Models</a>.
</li>

<li> Few-step Diffusion Models meetup, and the Diffusion Circle, at the International Conference of Machine Learning, 14th July 2025, Vancouver.
</li>

<li>Research Talk @ Genmo AI, Online, 3rd Jan, 2025: RLHF for Text-to-image Models and Beyond.
</li>

<li>Invited Talk @ Biomedical Engineering lab, Peking University, 25th Oct, 2024: Recent Progress on Diffusion Distillations.
</li>

<li>Invited Talk @ MAPLE lab, Westlake University, 20th Oct, 2024: Efficient Generative Models.
</li>
  
</ul>
      
      
<p><strong>News:</strong></p>

<ul>

  <li><strong>1st October 2025: </strong>one paper is public on <font color="red"><b>Arxiv</b></font><br>
    <b>Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct</b> <a href="https://arxiv.org/abs/2509.25035" target="\_blank">(Zheng et al., 2025)</a>.<br>
    <em> We introduce DiDi-Instruct, a very strong few-step language model that outperforms GPT2 (1024 NFE) and dLLMs (1024 NFE) with only 16 NFEs, at the same model size.</em>
  </li>
  
  <li><strong>18th September 2025: </strong>one paper is accepted by <font color="red"><b>NeurIPS 2025</b></font> @ San Diego and Mexican City.<br>
    <b>Reward-Instruct: A Reward-Centric Approach to Fast Photo-Realistic Image Generation</b> <a href="https://arxiv.org/abs/2503.13070" target="\_blank">(Luo et al., 2025)</a>.<br>
    <em> We present a novel finding: reward maximization with proper regularizations can effectively train large-scale few-step text-to-image generative models.</em>
  </li>

  <li><strong>18th September, 2025: </strong>one paper is accepted by <font color="red"><b>NeurIPS 2025</b></font> @ San Diego and Mexican City.<br>
    <b>Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction</b> <a href="https://arxiv.org/abs/2505.20755" target="\_blank">(Wang et al., 2025)</a>
    <em>Uni-Instruct unifies over 10 existing one-step diffusion distillation in theory, with an absolute SoTA one-step FID of 1.02 on ImageNet64 generation benchmark.</em>
  </li>

    <li><strong>12th September, 2025: </strong>one paper is accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (<font color="red"><b>TPAMI</b></font>).<br>
    <b>Self-Guidance: Boosting Flow and Diffusion Generation on Their Own</b> <a href="https://arxiv.org/abs/2412.05827" target="\_blank">(Li et al., 2024)</a>.<br>
    <em>Congratulations to my mentee student, Tiancheng, for getting a TPAMI acceptance in his first Ph.D. year.</em>
  </li>
  
  <li><strong>26th Agust, 2025: </strong>Introducing the dots.VLM1 by hi-lab: a large and versatile vision-language model built upon DeepseekV3 LLM architecture and an internal 1.2B MoE Vision Encoder. We train the VLM and VE from scratch, resulting in a model on par with leading VLMs on some metrics.<font color="red"><b>Arxiv</b></font>.<br>
    <b>Technical report of the dots.vlm1.</b> <a href="https://github.com/rednote-hilab/dots.vlm1/blob/main/README.md" target="\_blank">(hi-lab multimodal team)</a>
<!--     <em><a href="https://ai4scientificimaging.org/dive3d/" target="\_blank">Dive3D</a> introduces <a href="https://arxiv.org/abs/2410.16794" target="\_blank">Score-implicit Matching</a> techniques to text-to-3D generation, which significantly improves generative diversity as well as quality.</em> -->
  </li>
      
  <li><strong>16th June, 2025: </strong>one preprint paper is public on <font color="red"><b>Arxiv</b></font>.<br>
    <b>Dive3D: Diverse Distillation-based Text-to-3D Generation via Score Implicit Matching</b> <a href="https://arxiv.org/abs/2506.13594" target="\_blank">(Bai et al., 2025)</a>
    <em><a href="https://ai4scientificimaging.org/dive3d/" target="\_blank">Dive3D</a> introduces <a href="https://arxiv.org/abs/2410.16794" target="\_blank">Score-implicit Matching</a> techniques to text-to-3D generation, which significantly improves generative diversity as well as quality.</em>
  </li>
  
  <li><strong>25th May, 2025: </strong>one preprint paper is public on <font color="red"><b>Arxiv</b></font>.<br>
    <b>Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction</b> <a href="https://arxiv.org/abs/2505.20755" target="\_blank">(Wang et al., 2025)</a>
    <em>Uni-Instruct unifies over 10 existing one-step diffusion distillation in theory, with an absolute SoTA one-step FID of 1.02 on ImageNet64 generation benchmark.</em>
  </li>
  
  <li><strong>4th May, 2025: </strong>one paper accepted by <font color="red"><b>International Conference of Machine Learning (ICML) 2025</b></font>.<br>
    <b>Diff-Instruct*: Towards Human-Preferred One-step Text-to-image Generative Models</b> <a href="https://arxiv.org/abs/2410.20898" target="\_blank">(Luo et al., 2024)</a>
    <em>We introduced a novel score-based PPO algorithm for RL fine-tuning of 1-step text-to-image generative models. Our open-sourced 0.6B <a href="https://github.com/pkulwj1994/diff_instruct_star/blob/main/distar_1024.ipynb" target="\_blank">DIstar-SDXL-1step model</a> outperforms the 12B <a href="https://huggingface.co/black-forest-labs/FLUX.1-dev" target="\_blank">FLUX-dev diffusion model</a> in human preference scores.</em>
  </li>
  
  <li><strong>19th March 2025: </strong>one preprint paper is public on <font color="red"><b>Arxiv</b></font>.<br>
    <b>Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation</b> <a href="https://arxiv.org/abs/2503.13070" target="\_blank">(Luo et al., 2025)</a>.<br>
    <em> We present a novel finding: reward maximization with proper regularizations can effectively train large-scale few-step text-to-image generative models.</em>
  </li>

  <li><strong>27th February 2025: </strong>one paper accepted by <font color="red"><b>CVPR 2025</b></font>.<br>
    <b>Schedule On the Fly: Diffusion Time Prediction for Faster and Better Image Generation</b> <a href="https://arxiv.org/abs/2412.01243" target="\_blank">(Ye et al., 2024)</a>.<br>
    <em> We explore a novel attempt to use reinforcement learning for training diffusion models, with a very strong diffusion model with adaptive generation steps.</em>
  </li>

  <li><strong>23th January 2025:</strong> one paper accepted by <font color="red"><b>ICLR 2025</b></font>.<br>
      <a href="https://arxiv.org/abs/2406.14548" target="_blank">Consistency Models Made Easy</a>.<br>
      <em>We introduce a set of practical techniques for efficient training of consistency models, together with a comprehensive study on the Scaling Law of consistency models.</em>
  </li>

  <li><strong>5th Dec 2024: </strong> One pre-print is public on <font color="red"><b>Arxiv</b></font>.<br>
    <b>Self-Guidance: Boosting Flow and Diffusion Generation on Their Own</b> <a href="https://arxiv.org/abs/2412.05827" target="\_blank">(Li et al., 2024)</a>.<br>
    <em> <b>Self-guidance</b> can improve human hands and bodies of images generated by diffusion or flow models.</em>
  </li>

  <li><strong>1st Dec 2024: </strong> One pre-print is public on <font color="red"><b>Arxiv</b></font>.<br>
    <b>Schedule On the Fly: Diffusion Time Prediction for Faster and Better Image Generation</b> <a href="https://arxiv.org/abs/2412.01243" target="\_blank">(Ye et al., 2024)</a>.<br>
    <em> We introduced an approach for training variable-time-schedule diffusion models using reinforcement learning.</em>
  </li>

  <li><strong>21st Nov 2024: </strong> One <font color="red">single-author paper</font> accepted by <font color="red">Transactions on Machine Learning Research (<b>TMLR</b>)</font>.<br>
    <b>Diff-Instruct++: Training One-step Text-to-image Generator Model to Align with Human Preferences</b> <a href="https://arxiv.org/abs/2410.18881" target="\_blank">(Luo, 2024)</a>.<br>
    <em> <b>Diff-Instruct++</b> is the first work on preference alignment of one-step text-to-image generative models, opening the preference alignment with the distillation of diffusion and flow models.</em>
  </li>
  
  <li><strong>12th Nov 2024:</strong> Delivered an invited talk at the <font color="red"><strong>Google Deepmind Diffusion Reading Group</strong></font> titiled <strong>A Path to Human-preferred One-step Text-to-image Generative Models</strong>. Check the <a href="https://drive.google.com/file/d/1NrhHBKtVcZ8Jf2-I8v_QKz-Xsdvfqof0/view?usp=sharing"><strong>[Slides]</strong></a> here.
  </li>

  <li><strong>30th Oct 2024:</strong> Be invited to give an (internal) online academic talk in the <strong>Google Deepmind research</strong> team on 12th Nov. The talk title is <strong>One-step Text-to-image Generative Models: from Diffusion Distillation to Human-preference Alignment</strong>. In this talk, I will share some exciting progress in improving human preferences for one-step and few-step text-to-image generative models through the lens of Reinforcement Learning using Human Feedback (RLHF). Readers can refer to <a href="https://arxiv.org/abs/2410.18881" target="\_blank">Diff-Insruct++</a> and <a href="https://arxiv.org/abs/2410.20898" target="\_blank">Diff-Insruct*</a> for technical details. 
  </li>
  
  <li><strong>25th Oct 2024:</strong> An invited talk delivered at the Biomedical Engineering lab led by <a href="http://users.cms.caltech.edu/~hesun/">Dr. Sun</a> at Peking University, Beijing, China. The talk is on <strong>Recent Progresses on Diffusion Distillation</strong>.
  </li>
  
  <li><strong>20th Oct 2024:</strong> Had an academic visit to MAPLE lab led by <a href="https://www.westlake.edu.cn/faculty/guojun-qi.html" target="\_blank">Dr. Qi</a> in Westlake University, Hangzhou, China. Delivered a talk on <strong>Efficient Generative Models</strong> to lab members.
  </li>

  <li><strong>18th Oct 2024:</strong> one reprint released on <font color="red"><b>Arxiv</b></font>.<br>
    <b>One-step Flow Matching Generators</b> <a href="https://arxiv.org/abs/2410.19310" target="\_blank">(Huang et al., 2024)</a>.<br>
    <em>We introduce a novel method to distill the flow-matching-based Stable Diffusion 3 model into strong one-step generators.</em>
  </li>

  <li><strong>18th Oct 2024:</strong> one reprint released on <font color="red"><b>Arxiv</b></font>.<br>
    <b>Diff-Instruct*: Towards Human-Preferred One-step Text-to-image Generative Models</b> <a href="https://arxiv.org/abs/2410.20898" target="\_blank">(Luo et al., 2024)</a>.<br>
    <em>This paper introduces the <b>Diff-Instruct*</b>, a novel approach to train human-preferred large-scale one-step text-to-image generative models through the lens of online RLHF with general score-based constraints. The resulting one-step 0.6B DiT-DI* model achieves a SoTA <a href="https://github.com/tgxs002/HPSv2" target="\_blank">HPSv2.0</a> score of <b>28.70</b>.</em>
  </li>

  <li><strong>17th Oct 2024:</strong> one reprint released on <font color="red"><b>Arxiv</b></font>.<br>
    <b>Diff-Instruct++: Training One-step Text-to-image Generator Model to Align with Human Preferences</b> <a href="https://arxiv.org/abs/2410.18881" target="\_blank">(Luo, 2024)</a>.<br>
    <em>This paper introduces the <b>Diff-Instruct++</b>, the first attempt at human preference alignment of large-scale one-step text-to-image generative models. The aligned one-step 0.6B DiT-DI++ model achieves a leading <a href="https://github.com/tgxs002/HPSv2" target="\_blank">HPSv2.0</a> score of <b>28.48</b>.</em>
  </li>
  
  <li><strong>14th Oct 2024:</strong><font color="red">I defended my PhD Thesis</font> in 14th Oct in <a href="http://english.pku.edu.cn">Peking University</a>. I feel humbled and grateful to be loved and helped by great advisors, family, and awesome friends.</li>
  
  <li><strong>26th Sep 2024:</strong><font color="red">one paper accepted by <b>NeurIPS 2024</b></font>.<br>
    <b>One-step Diffusion Distillation Through Score Implicit Matching</b> <a href="https://arxiv.org/abs/2410.16794" target="\_blank">(Luo et al., NeurIPS 2024)</a>.<br>
    <em>We introduce the score implicit matching, a novel one-step diffusion distillation approach with an amazing one-step text-to-image generative model. Appreciation to Prof. Zico Kolter and Prof. Guojun Qi.</em>
  </li>

  <li><strong>20th Jun 2024:</strong> one preprint released on <font color="red"><b>Arxiv</b></font>.<br>
      <b>Consistency Models Made Easy</b> <a href="https://arxiv.org/abs/2406.14548" target="_blank">(Geng et al., 2024)</a>.<br>
      <em>We introduce a set of practical techniques for efficient training of consistency models, together with a comprehensive study on the Scaling Law of consistency models.</em>
  </li>

  <li><strong>24th Apr 2024:</strong><font color="red">one paper accepted by <b>ICML 2024</b></font>.<br>
      <b>Variational Schrödinger Diffusion Models</b> <a href="https://openreview.net/forum?id=kRv0WPJd00" target="_blank">(Deng et al., ICML 2024)</a>.<br>
      <em>We introduce an efficient simulation-free Schrödinger diffusion model, with wide applications for image and time-series generation. Congratulations to Yixin and Dr. Deng.</em>
  </li>

  <li><strong>26th Sep 2023:</strong><font color="red">oone paper accepted by <b>NeurIPS 2023</b></font>.<br>
      <b>Diff-instruct: A Universal Approach for Transferring Knowledge from Pre-trained Diffusion Models</b> <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/f115f619b62833aadc5acb058975b0e6-Abstract-Conference.html" target="_blank">(Luo et al., NeurIPS 2023)</a>.<br>
      <em>Diff-Instruct is a one-step diffusion distillation approach through the lens of distribution matching, with applications on text-to-3D generation and improving GAN generators.</em>
  </li>

  <li><strong>26th Sep 2023:</strong><font color="red">one paper accepted by <b>NeurIPS 2023</b></font>.<br>
      <b>Entropy-based Training Methods for Scalable Neural Implicit Samplers</b> <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/1646e34971facbcda3727d1dc28ab635-Abstract-Conference.html" target="_blank">(Luo et al., NeurIPS 2023)</a>.<br>
      <em>We introduced two interesting training approaches for neural implicit samplers termed KL and Fisher training.</em>
  </li>

  <li><strong>26th Sep 2023:</strong><font color="red">one paper accepted by <b>NeurIPS 2023</b></font>.<br>
      <b>SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models</b> <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/f4a6806490d31216a3ba667eb240c897-Abstract-Conference.html" target="_blank">(Xue et al., NeurIPS 2023)</a>.<br>
      <em>We introduced a novel diffusion sampler based on the Stochastic Adam theory, integrated for PixelArt-alpha diffusion models.</em>
  </li>

  <li><strong>26th Sep 2023:</strong><font color="red">one paper accepted by <b>NeurIPS 2023</b></font>.<br>
      <b>Enhancing Adversarial Robustness via Score-based Optimization</b> <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/a2e707354da36956945dbb288efe82b3-Abstract-Conference.html" target="_blank">(Zhang et al., NeurIPS 2023)</a>.<br>
      <em>We introduced a novel optimization-based adversarial defense based on pre-trained diffusion models.</em>
  </li>

  <li><strong>9th Apr 2023:</strong> one paper released on <font color="red"><b>Arxiv</b></font>.<br>
      <b>A Comprehensive Survey on Knowledge Distillation of Diffusion Models</b> <a href="https://arxiv.org/abs/2304.04262" target="_blank">(Luo, 2023)</a>.<br>
      <em>The first survey on diffusion distillation and knowledge transferring of diffusion models.</em>
  </li>
</ul>


<p><strong>Friends with whom I have worked on projects:</strong></p>

<ul>
  <li><strong><a href="https://zicokolter.com/" target="_blank">J. Zico Kolter</a></strong>, Professor, Director of the Machine Learning Department, Carnegie Mellon University (CMU).<br>
  </li>
  <li><strong><a href="https://maple.lab.westlake.edu.cn/" target="_blank">Guo-jun Qi</a></strong>, Professor, IEEE Fellow, Director of MAPLE Lab of Westlake University.<br>
  </li>
  <li><strong><a href="https://www.linkedin.com/in/guang-lin-7a464624/" target="_blank">Guang Lin</a></strong>, Associate Dean for Research, Moses Cobb Stevens Professor in Mathematics, Mech Eng<a href="https://engineering.purdue.edu/ME" target="_blank"> Purdue University</a>.<br>
  </li>
  <li><strong><a href="https://ml.comp.nus.edu.sg/kawaguchi" target="_blank">Kenji Kawaguchi</a></strong>, Presidential Young Professor at Department of Computer Science<a href="https://ml.comp.nus.edu.sg/#" target="_blank"> National University of Singapore</a>.<br>
  </li>
  <li><strong><a href="https://zhenguol.github.io/" target="_blank">Zhenguo Li</a></strong>, director of the AI Theory Lab in <a href="https://www.noahlab.com.hk/#/home" target="_blank">Huawei Noah’s Ark Lab, Hongkong</a>, Adjunct Professor in the department of computer science and engineering, <a href="https://hkust.edu.hk/zh-hant" target="_blank">Hong Kong University of Science and Technology</a>.<br>
  </li>
  <li><strong><a href="https://ai4scientificimaging.org/" target="_blank">He Sun</a></strong>, PhD, tenure-track Assistant Professor at <a href="https://english.pku.edu.cn/" target="_blank">Peking University</a>.<br>
  </li>
  <li><strong><a href="https://hu-tianyang.github.io/" target="_blank">Tianyang Hu</a></strong>, PhD, Incoming Assistant Professor at the Chinese University of Hong Kong, Shenzhen (<a href="https://sds.cuhk.edu.cn/en" target="_blank">CUHK-Shenzhen</a>).<br>
  </li>
  <li><strong><a href="https://wenzhengchen.github.io/" target="_blank">Wenzheng Chen</a></strong>, PhD, tenure-track Assistant Professor at <a href="https://english.pku.edu.cn/" target="_blank">Peking University</a>.<br>
  </li>
  <li><strong><a href="https://www.weideng.org/" target="_blank">Wei Deng</a></strong>, PhD, Senior Research Scientist at Morgan Stanley, New York.<br>
  </li>
  <li><strong><a href="https://rtqichen.github.io/" target="_blank">Ricky Tian Qi Chen</a></strong>, PhD, Research Scientist at Meta Fundamental AI Research (FAIR), New York.<br>
  </li>
  <li><strong>Zheyuan Hu</strong>, PhD from the National University of Singapore, the Winner of the <strong>NeurIPS 2024 best paper award</strong>.<br>
  </li>
  <li><strong><a href="https://www.linkedin.com/in/sethforsgren/" target="_blank">Seth Forsgren</a></strong>, BS from Princeton, CEO and the founder of <a href="https://www.riffusion.com/" target="_blank">Riffusion AI</a>, San Francisco.<br>
  </li>
  <li><strong><a href="https://www.haykmartiros.com/" target="_blank">Hayk Martiros</a></strong>, MS from Stanford, CTO and the co-founder of <a href="https://www.riffusion.com/" target="_blank">Riffusion AI</a>, technical VP of <a href="https://www.skydio.com/" target="_blank">Skydio</a>.<br>
  </li>
  <li><strong><a href="" target="_blank">Debing Zhang</a></strong>, PhD, Director of Artificial General Intelligence (AGI) team of <a href="https://x.com/xiaohongshu" target="_blank">RedNote</a>, aka Xiaohongshu Inc.<br>
  </li>
</ul>
  
  
<p><strong>Previous students whom I have advised or worked with:</strong></p>
<ul>
  <li><strong><a href="" target="_blank">Weimin Bai</a></strong>, PhD student at Peking University, co-advised with Professor He Sun.<br>
  <li><strong><a href="" target="_blank">Haoyang Zheng</a></strong>, PhD student at Purdue University, co-advised with Professor Guang Lin.<br>
  <li><strong><a href="" target="_blank">Yubo Li</a></strong>, Undergraduate. student at Tsinghua University, and incoming PhD student at Peking University, co-advised by Professor He Sun.<br>
  <li><strong><a href="" target="_blank">Yifei Wang</a></strong>, Undergraduate student at Peking University, and incoming PhD student at Rice University (Houston). Co-advised with Professor He Sun.<br>
  <li><strong><a href="" target="_blank">Le Zhuo</a></strong>, Incoming CS PhD student of <a href="https://mmlab.ie.cuhk.edu.hk/index.html" target="_blank">MMLab</a> at the Chinese University of Hong Kong.<br>
  <li><strong><a href="" target="_blank">Zemin Huang</a></strong>, CS PhD student of the joint PhD Program of Zhejiang University and Westlake University, co-advised with Professor Guo-jun Qi.<br>
  <li><strong><a href="" target="_blank">Tiancheng Li</a></strong>, CS PhD student of the joint PhD Program of Zhejiang University and Westlake University, co-advised with Professor Guo-jun Qi.<br>
  <li><strong><a href="" target="_blank">Zilyu Ye</a></strong>, CS undergraduate student of South China University of Technology, ByteDance Top Seed Intern, co-advised with Professor Guo-jun Qi.<br>
  <li><strong><a href="" target="_blank">Yuxuan Gu</a></strong>, Incoming M.S. student at Peking University, co-advised with Professor He Sun.<br>
  <li><strong><a href="" target="_blank">Chaowei Liu</a></strong>, National University of Singapore (NUS), co-advised with Professor Guo-jun Qi.<br>
</ul>
  
  </article>

</div>

    </div>

    <!-- Footer -->

    

  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-180825462-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'UA-180825462-1');
</script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
